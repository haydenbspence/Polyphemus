---
title: "Background"
---

# Introduction

Large Language Models (LLMs) represent a cutting-edge advancement in the field of artificial intelligence, specifically within natural language processing (NLP). These models are designed to understand and generate human-like text, exhibiting an impressive capacity for language comprehension and creation. Built on the principles of machine learning, LLMs, like OpenAI's GPT-3 or GPT-4, are trained on vast amounts of data, allowing them to predict and generate coherent and contextually appropriate responses. From enhancing user experiences in chatbots, to generating content, and assisting with language translation, LLMs are revolutionizing how we interact with technology, offering an increasingly nuanced interface that blends AI sophistication with the complexities of human language.

# Timeline

Our objective in presenting this timeline of events is to highlight the swift progress and growing democratization in the field of open-source AI and application of LLMs. In doing so, we hope to demonstrate the feasibility and opportunities for applications in the area of health informatics and data science. This timeline primarily focuses on open-source technologies and open-science research that have practical, working examples, rather than emphasizing cutting-edge institutional research breakthroughs or proprietary advancements.\

+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Date       | Event                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Significance and Implications                                                                                                                                                                                                                                                                                                                                                                                   |
+============+===============================================================================================================================================================================================================================================================================================================================================================================================================================================================+=================================================================================================================================================================================================================================================================================================================================================================================================================+
| 2022-10-17 | [Chase Harrison releases LangChain](https://github.com/hwchase17/langchain/blob/master/CITATION.cff), a python library that assists in the development of applications that combine LLMs and other sources of knowledge.                                                                                                                                                                                                                                      | LangChain lowers the barrier to integrating disparate sources and discrete LLMs. This allows for interdisciplinary and more sophisticated networking of data sources and LLMs.                                                                                                                                                                                                                                  |
|            |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                 |
|            | "We believe that the most powerful and differentiated applications will not only call out to a language model, but will also be *Data-aware* and *Agentic*.                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-02-24 | [Meta launches Large Language Model Meta AI (LLaMA) and open sources code without model weights.](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)                                                                                                                                                                                                                                                                                           | Meta's decision to release all of the source code for their AI, while withholding the weights, does illustrate a degree of commitment to open-source principles among major tech firms. However, the exclusion of the weights from this release signifies a cautious approach, emphasizing the proprietary value that these weights contribute to the effectiveness of LLMs.                                    |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-03-03 | [LLaMA weights are leaked and shared via torrent on the internet.](https://web.archive.org/web/20230603194841/https://www.vice.com/en/article/xgwqgw/facebooks-powerful-large-language-model-leaks-online-4chan-llama)                                                                                                                                                                                                                                        | The leak of Meta's LLaMA weights raises the potential of misuse of AI technology, disrupts the control and security of this technology, highlights legal issues. Importantly, it also spurs AI research and development by universities and open-source communities.                                                                                                                                            |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-03-12 | [Artem Andreenko is able to get LLaMA working the a Raspberry Pi](https://github.com/ggerganov/llama.cpp/issues/58)                                                                                                                                                                                                                                                                                                                                           | Andreenko's report of being able to run LLaMA on inexpensive consumer-level hardware like the Raspberry Pi 4 demonstrates the potential for the democratization of AI. High-end resources present a barrier to who can implement, and especially who can fine-tune, LLMs. Lowering this barrier means localized solutions, hobbyist tinkering, and innovation will involve a broader segment of the population. |
|            |                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                 |
|            |                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Although the implementation of LLMs on this level of hardware is slow and impractical for the majority of use-cases, the energy efficiency aspect is also promising as AI tends to have high energy consumption.                                                                                                                                                                                                |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-03-13 | [Stanford releases Alpaca, the first minimal working example of fine-tuning LLaMA](https://crfm.stanford.edu/2023/03/13/alpaca.html)                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-03-14 | Eric Wang releases [Alpaca-LoRA](https://github.com/tloen/alpaca-lora)                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-03-19 | The [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) project trained a model on high-quality ChatGPT dialogues sourced from sites such as [ShareGPT](https://sharegpt.com/).                                                                                                                                                                                                                                                                               | Cost of Training: \~\$300                                                                                                                                                                                                                                                                                                                                                                                       |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-03-25 | Nomic releases GPT4All                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-03-03 | [Cerebras](https://www.cerebras.net/) trained the GPT-3 architecture using the optimal compute schedule implied by Chinchilla and the optimal scaling implied by [μ-parameterization](https://arxiv.org/abs/2203.03466). This marked the first example of a model that outperformed GPT-3 and was trained from scratch. Consequently, the open source community gained full access to two fLLMs that rival GPT4 and PaLM2.                                    |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-03-28 | LLaMA-Adapter uses Parameter Efficient Fine Tuning (PEFT), the project introduced instruction tuning and multimodality in a mere hour of training, establishing a new state-of-the-art (SOTA) for Science Q&A.                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-04-03 | The [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) project released a model trained on open source data. When evaluated against ChatGPT with human subjects, over 50% of users either preferred Koala responses or expressed no preference between the two. The training cost was approximately \$100.                                                                                                                                             | Cost of Training: \~\$100                                                                                                                                                                                                                                                                                                                                                                                       |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 2023-04-15 | The Open Assistant project launched a [model and dataset ](https://arxiv.org/pdf/2304.07327.pdf)for Alignment via Real Life Human Feedback (RLHF). Their model closely competed with ChatGPT in terms of human preference (48.3% vs. 51.7%). The dataset could also be applied to Pythia-12B, and a complete open-source technology stack was provided to run the model. This publicly available dataset made RLHF feasible for individual users.             | Cost of Training: \~\$100                                                                                                                                                                                                                                                                                                                                                                                       |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|            | [LLamaIndex](https://github.com/jerryjliu/llama_index)                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|            | GPTCache                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|            | LocalAI                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|            | PandasAI                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|            | GPT-Q for LLaMA                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|            | Large Language Model Activation-aware Weight Quantization (LLM-AWQ)                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|            | Microsoft Guidance                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|            | Simple-LLM-Finetuner                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|            | The Paper [*Generative Agents: Interactive Simulacra of Human Behavior*](https://arxiv.org/pdf/2304.03442.pdf)is implemented by [Pham Tran Minh Quang](https://quangbk.github.io/) using LangChain and uploaded to GitHub ([generativeAgent_LLM](https://github.com/QuangBK/generativeAgent_LLM)) with an accompanying [article](https://betterprogramming.pub/implement-generative-agent-with-local-llm-guidance-and-langchain-full-features-fa57655f3de1).  |                                                                                                                                                                                                                                                                                                                                                                                                                 |
+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+