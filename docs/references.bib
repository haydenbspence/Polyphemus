
@misc{patel_google_nodate,
	title = {Google "{We} {Have} {No} {Moat}, {And} {Neither} {Does} {OpenAI}"},
	url = {https://www.semianalysis.com/p/google-we-have-no-moat-and-neither},
	abstract = {Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI},
	language = {en},
	urldate = {2023-05-26},
	author = {Patel, Dylan},
	file = {Snapshot:C\:\\Users\\diana\\Zotero\\storage\\UJF8BK9G\\google-we-have-no-moat-and-neither.html:text/html},
}

@misc{anishmaxxing__thiteanish_llamas_2023,
	type = {Tweet},
	title = {Llama's running at 5 token / sec on a {Pixel} 6 thanks to @koansin https://t.co/{ZCGbt3X9RE}},
	url = {https://twitter.com/thiteanish/status/1635678053853536256},
	language = {en},
	urldate = {2023-05-26},
	journal = {Twitter},
	author = {{anishmaxxing üè¥‚Äç‚ò†Ô∏è [@thiteanish]}},
	month = mar,
	year = {2023},
}

@misc{wang__2023,
	title = {ü¶ôüå≤ü§è {Alpaca}-{LoRA}},
	copyright = {Apache-2.0},
	url = {https://github.com/tloen/alpaca-lora},
	abstract = {Instruct-tune LLaMA on consumer hardware},
	urldate = {2023-05-26},
	author = {Wang, Eric J.},
	month = may,
	year = {2023},
	note = {original-date: 2023-03-13T21:52:36Z},
}

@article{zhang_llama-adapter_2023,
	title = {Llama-adapter: {Efficient} fine-tuning of language models with zero-init attention},
	journal = {arXiv preprint arXiv:2303.16199},
	author = {Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
	year = {2023},
	keywords = {‚õî No DOI found},
	file = {Zhang et al_2023_Llama-adapter.pdf:C\:\\Users\\diana\\Zotero\\storage\\4KWWJCQP\\Zhang et al_2023_Llama-adapter.pdf:application/pdf},
}

@article{kopf_openassistant_2023,
	title = {{OpenAssistant} {Conversations}‚Äì{Democratizing} {Large} {Language} {Model} {Alignment}},
	journal = {arXiv preprint arXiv:2304.07327},
	author = {K√∂pf, Andreas and Kilcher, Yannic and von R√ºtte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Rich√°rd and {others}},
	year = {2023},
	keywords = {‚õî No DOI found},
	file = {K√∂pf et al_2023_OpenAssistant Conversations‚ÄìDemocratizing Large Language Model Alignment.pdf:C\:\\Users\\diana\\Zotero\\storage\\LEII7WRR\\K√∂pf et al_2023_OpenAssistant Conversations‚ÄìDemocratizing Large Language Model Alignment.pdf:application/pdf},
}

@misc{noauthor_vicuna_nodate,
	title = {Vicuna: {An} {Open}-{Source} {Chatbot} {Impressing} {GPT}-4 with 90\%* {ChatGPT} {Quality} {\textbar} {LMSYS} {Org}},
	shorttitle = {Vicuna},
	url = {https://lmsys.org/blog/2023-03-30-vicuna},
	abstract = {{\textless}p{\textgreater}We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation ...},
	language = {en},
	urldate = {2023-05-26},
	file = {Snapshot:C\:\\Users\\diana\\Zotero\\storage\\APM2MVZB\\2023-03-30-vicuna.html:text/html},
}

@misc{gerganov_llamacpp_2023,
	title = {llama.cpp},
	copyright = {MIT},
	url = {https://github.com/ggerganov/llama.cpp},
	abstract = {Port of Facebook's LLaMA model in C/C++},
	urldate = {2023-05-26},
	author = {Gerganov, Georgi},
	month = may,
	year = {2023},
	note = {original-date: 2023-03-10T18:58:00Z},
}

@misc{sourab_mangrulkar_peft_2022,
	title = {{PEFT}: {State}-of-the-art {Parameter}-{Efficient} {Fine}-{Tuning} methods},
	url = {https://github.com/huggingface/peft},
	author = {Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul},
	year = {2022},
}

@article{hu_lora_2021,
	title = {Lora: {Low}-rank adaptation of large language models},
	journal = {arXiv preprint arXiv:2106.09685},
	author = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	year = {2021},
	keywords = {‚õî No DOI found},
	file = {Hu et al_2021_Lora.pdf:C\:\\Users\\diana\\Zotero\\storage\\VCLVFINS\\Hu et al_2021_Lora.pdf:application/pdf},
}

@misc{noauthor_gpt4all_2023,
	title = {{GPT4All}},
	copyright = {MIT},
	url = {https://github.com/nomic-ai/gpt4all},
	abstract = {gpt4all: an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue},
	urldate = {2023-05-26},
	publisher = {Nomic AI},
	month = may,
	year = {2023},
	note = {original-date: 2023-03-27T18:49:32Z},
}

@misc{anand_gpt4all_2023,
	title = {{GPT4All}: {Training} an {Assistant}-style {Chatbot} with {Large} {Scale} {Data} {Distillation} from {GPT}-3.5-{Turbo}},
	author = {Anand, Yuvanesh and Nussbaum, Zach and Duderstadt, Brandon and Schmidt, Benjamin and Mulyar, Andriy},
	year = {2023},
}

@article{yang_tensor_2022,
	title = {Tensor programs v: {Tuning} large neural networks via zero-shot hyperparameter transfer},
	journal = {arXiv preprint arXiv:2203.03466},
	author = {Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
	year = {2022},
	keywords = {‚õî No DOI found},
	file = {Yang et al_2022_Tensor programs v.pdf:C\:\\Users\\diana\\Zotero\\storage\\QSCIZQBX\\Yang et al_2022_Tensor programs v.pdf:application/pdf},
}

@misc{noauthor_bloom_nodate,
	title = {{BLOOM}},
	url = {https://bigscience.huggingface.co/blog/bloom},
	abstract = {Our 176B parameter language model is here.},
	urldate = {2023-05-26},
	file = {Snapshot:C\:\\Users\\diana\\Zotero\\storage\\XFA8GU9S\\bloom.html:text/html},
}

@misc{seita_koala_nodate,
	title = {Koala: {A} {Dialogue} {Model} for {Academic} {Research}},
	shorttitle = {Koala},
	url = {http://bair.berkeley.edu/blog/2023/04/03/koala/},
	abstract = {The BAIR Blog},
	urldate = {2023-05-26},
	journal = {The Berkeley Artificial Intelligence Research Blog},
	author = {Seita, Daniel and Song, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, Dawn, Xinyang Geng},
	file = {Snapshot:C\:\\Users\\diana\\Zotero\\storage\\SZYJF5S4\\koala.html:text/html},
}

@misc{noauthor_open_nodate,
	title = {Open {Assistant}},
	url = {https://open-assistant.io/},
	urldate = {2023-05-26},
	file = {Open Assistant:C\:\\Users\\diana\\Zotero\\storage\\B9EZPDCU\\open-assistant.io.html:text/html},
}

@misc{noauthor_sharegpt_nodate,
	title = {{ShareGPT}: {Share} your wildest {ChatGPT} conversations with one click.},
	shorttitle = {{ShareGPT}},
	url = {https://sharegpt.com},
	abstract = {ShareGPT is a Chrome extension that allows you to share your wildest ChatGPT conversations with one click.},
	language = {en},
	urldate = {2023-05-26},
	file = {Snapshot:C\:\\Users\\diana\\Zotero\\storage\\S6LNAPA6\\sharegpt.com.html:text/html},
}

@misc{noauthor_stanford_nodate,
	title = {Stanford {CRFM}},
	url = {https://crfm.stanford.edu/2023/03/13/alpaca.html},
	urldate = {2023-05-26},
	file = {Stanford CRFM:C\:\\Users\\diana\\Zotero\\storage\\5INL92HQ\\alpaca.html:text/html},
}

@misc{noauthor_raspberry_nodate,
	title = {Raspberry {Pi} 4 {4GB} ¬∑ {Issue} \#58 ¬∑ ggerganov/llama.cpp},
	url = {https://github.com/ggerganov/llama.cpp/issues/58},
	abstract = {Hi! Just a report. I've successfully run the LLaMA 7B model on my 4GB RAM Raspberry Pi 4. It's super slow at about 10 sec/token. But it looks like we can run powerful cognitive pipelines on a cheap...},
	language = {en},
	urldate = {2023-05-26},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\diana\\Zotero\\storage\\TYLSNUXM\\58.html:text/html},
}

@misc{noauthor_nebulyoptimizationchatllama_nodate,
	title = {nebuly/optimization/chatllama at main ¬∑ nebuly-ai/nebuly},
	url = {https://github.com/nebuly-ai/nebuly},
	abstract = {The next-generation platform to monitor and optimize your AI costs in one place üöÄ - nebuly/optimization/chatllama at main ¬∑ nebuly-ai/nebuly},
	language = {en},
	urldate = {2023-05-26},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\diana\\Zotero\\storage\\VCXI78TT\\chatllama.html:text/html},
}

@misc{noauthor_ggerganovllamacpp_nodate,
	title = {ggerganov/llama.cpp: {Port} of {Facebook}'s {LLaMA} model in {C}/{C}++},
	url = {https://github.com/ggerganov/llama.cpp},
	urldate = {2023-05-26},
	file = {ggerganov/llama.cpp\: Port of Facebook's LLaMA model in C/C++:C\:\\Users\\diana\\Zotero\\storage\\XRLYR653\\llama.html:text/html},
}

@misc{noauthor_freedomgpt_nodate,
	title = {{FreedomGPT}},
	url = {https://freedomgpt.com/},
	urldate = {2023-05-26},
	file = {FreedomGPT:C\:\\Users\\diana\\Zotero\\storage\\JX5CM9M7\\freedomgpt.com.html:text/html},
}

@misc{noauthor_colossal-ai_2023,
	title = {Colossal-{AI}},
	copyright = {Apache-2.0},
	url = {https://github.com/hpcaitech/ColossalAI},
	abstract = {Making large AI models cheaper, faster and more accessible},
	urldate = {2023-05-26},
	publisher = {HPC-AI Tech},
	month = may,
	year = {2023},
	note = {original-date: 2021-10-28T16:19:44Z},
	keywords = {ai, big-model, data-parallelism, deep-learning, distributed-computing, foundation-models, heterogeneous-training, hpc, inference, large-scale, model-parallelism, pipeline-parallelism},
}

@misc{patel_google_nodate-1,
	title = {Google "{We} {Have} {No} {Moat}, {And} {Neither} {Does} {OpenAI}"},
	url = {https://www.semianalysis.com/p/google-we-have-no-moat-and-neither},
	abstract = {Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI},
	language = {en},
	urldate = {2023-05-26},
	author = {Patel, Dylan},
	file = {Snapshot:C\:\\Users\\diana\\Zotero\\storage\\NQ4X4LIG\\google-we-have-no-moat-and-neither.html:text/html},
}
