{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\Users\\admin\\Polyphemus\\venv\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so\n",
      "CUDA SETUP: Loading binary c:\\Users\\admin\\Polyphemus\\venv\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n",
      "argument of type 'WindowsPath' is not iterable\n",
      "Training Alpaca-LoRA model with params:\n",
      "base_model: decapoda-research/llama-7b-hf\n",
      "data_path: yahma/alpaca-cleaned\n",
      "output_dir: ./lora-alpaca\n",
      "batch_size: 128\n",
      "micro_batch_size: 8\n",
      "num_epochs: 10\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 512\n",
      "val_set_size: 2000\n",
      "lora_r: 16\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: [q_proj,k_proj,v_proj,o_proj]\n",
      "train_on_inputs: True\n",
      "add_eos_token: False\n",
      "group_by_length: True\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: alpaca\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Polyphemus\\venv\\Lib\\site-packages\\bitsandbytes\\cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\admin\\Polyphemus\\finetune.py\", line 269, in <module>\n",
      "    fire.Fire(train)\n",
      "  File \"c:\\Users\\admin\\Polyphemus\\venv\\Lib\\site-packages\\fire\\core.py\", line 141, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\admin\\Polyphemus\\venv\\Lib\\site-packages\\fire\\core.py\", line 475, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\admin\\Polyphemus\\venv\\Lib\\site-packages\\fire\\core.py\", line 691, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\admin\\Polyphemus\\finetune.py\", line 112, in train\n",
      "    model = LlamaForCausalLM.from_pretrained(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\admin\\Polyphemus\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 2722, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: \n",
      "                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n",
      "                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n",
      "                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n",
      "                        `device_map` to `from_pretrained`. Check\n",
      "                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n",
      "                        for more details.\n",
      "                        \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "# Activate the virtual environment\n",
    "virtualenv_path = 'venv'  # Replace with the path to your virtual environment\n",
    "activate_script = os.path.join(virtualenv_path, 'Scripts', 'activate.ps1')\n",
    "\n",
    "\n",
    "# Set environment variables\n",
    "env = os.environ.copy()\n",
    "env['WANDB_PROJECT'] = 'Polyphemus'\n",
    "env['CUDA_PATH'] = 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.0'\n",
    "env['LD_LIBRARY_PATH'] = 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.0\\\\lib\\\\x64'\n",
    "\n",
    "\n",
    "!python finetune.py --base_model='decapoda-research/llama-7b-hf' --num_epochs=10 --cutoff_len=512 --group_by_length --output_dir='./lora-alpaca' --lora_target_modules='[q_proj,k_proj,v_proj,o_proj]' --lora_r=16 --micro_batch_size=8\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
